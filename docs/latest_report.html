<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Coach Interaction Refresh</title>
  <style>
    body { font-family: "SF Pro Text", "Helvetica Neue", Arial, sans-serif; margin: 24px; line-height: 1.6; color: #111; }
    h1 { font-size: 26px; margin-bottom: 8px; }
    h2 { margin-top: 28px; font-size: 18px; }
    code { background: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    pre { background: #f9fafb; padding: 12px; border-radius: 10px; overflow-x: auto; }
    .pill { display: inline-block; padding: 4px 10px; border-radius: 20px; background: #e8f1ff; color: #1f4b99; font-weight: 600; }
  </style>
</head>
<body>
  <h1>Coach Interaction Refresh <span class="pill">AI Assisted Narration</span></h1>

  <h2>ğŸ¦… Bird's Eye View</h2>
  <p>The realtime logic still lives in <code>Moment/Moment/OpenAIRealtimeCoachService.swift</code>, but the prompt builder now injects an explicit â€œbridge back to the noteâ€ directive so every question ties the last spoken sentence to the note snapshot. On the presentation layer, <code>Moment/Moment/NarrationCoachView.swift</code> mirrors the reference mock (prompt card + CTA, summary chip at the end), while <code>Moment/Moment/LiveSpeechRecognizer.swift</code> + <code>NarrationCoachViewModel</code> collaborate so that each 1â€“2s pause immediately triggers a new guiding question.</p>

  <h2>ğŸ§© Architecture & State</h2>
  <ul>
    <li><strong>Prompt Strategy:</strong> The warmup/follow-up instructions stress: â€œquote the userâ€™s latest keywords, then point to the next unresolved note bullet.â€ This guarantees continuity even when the narration stalls.</li>
    <li><strong>Continuous Listening:</strong> The speech recognizer keeps the audio engine running and respawns <code>SFSpeechRecognitionTask</code> after each final result, feeding a silence monitor in the view model.</li>
    <li><strong>Silence Monitor:</strong> <code>NarrationCoachViewModel</code> tracks the last utterance timestamp; if the user stays quiet for â‰¥1.8s, it sends the most recent partial/final sentence to the realtime service to get the next bridging prompt.</li>
    <li><strong>UI State:</strong> The view now binds only to <code>currentPrompt</code>, <code>state</code>, and <code>summaryText</code>. Transcript rows were removed, reducing noise and highlighting the guidance card.</li>
    <li><strong>Control Loop:</strong> The circular mic button still toggles <code>viewModel.toggleSession()</code>; the rest of the scaffolding (alerts, copy toast) remains intact but visually secondary.</li>
  </ul>

  <h2>âš—ï¸ The Flow</h2>
  <pre>
User taps â€œå¼€å§‹ç»ƒä¹ â€
        |
        v
NarrationCoachViewModel
  - starts speech recognizer
  - streams transcript chunks
        |
        v
OpenAIRealtimeCoachService
  - session.update with note snapshot & language
  - response.create per warmup/follow-up with bridging directive
        |
        v
LiveSpeechRecognizer
  - streams partial text continuously
  - view model silence monitor waits ~1.8s, then calls follow-up prompt with latest speech
        |
        v
OpenAI Realtime (gpt-realtime-mini-2025-10-06)
  - emits response.text.delta ... response.done
        |
        v
Prompt card updates inline, summary card appears after â€œç»“æŸç»ƒä¹ â€
  </pre>

  <h2>ğŸ•¹ Usage</h2>
  <ul>
    <li>Tap â€œå¼€å§‹ç»ƒä¹ â€ï¼šthe card immediately streams the first bridging hint that references the note.</li>
    <li>Whenever you pause, the next question reuses your last words to pull in the next note bullet so you never lose context.</li>
    <li>Tap â€œç»“æŸç»ƒä¹ â€ï¼šthe coach summarizes the full narration and the summary card is revealed with a copy action.</li>
  </ul>

  <h2>ğŸ› ï¸ Realtime API Fix Retrospective</h2>
  <p>Earlier we kept seeing the â€œThe server had an error while processing your requestâ€ dialog because our websocket payloads were being sent as binary frames (<code>.data</code>) while the OpenAI realtime endpoint expects UTFâ€‘8 text frames. The server accepted the handshake but failed every <code>session.update</code>/<code>response.create</code>, which surfaced as a generic server error in the UI.</p>
  <p>The fix lives in <code>OpenAIRealtimeCoachService.send(json:using:)</code>: we now serialize JSON to text and send via <code>.string</code>, plus we normalized event handling to the new <code>response.text.delta/response.done</code> schema. After that change, the standalone probe (documented in <code>custom-rules.mdc</code>) consistently receives streamed deltas, and the app no longer surfaces the previous error alert.</p>
</body>
</html>
